{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in Machine Learning Models\n",
    "\n",
    "**Yuanzhe(Roger) Li, 2020-01-07**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Outline\n",
    "- Regularized Regression\n",
    "    - Ridge Regression\n",
    "    - LASSO\n",
    "- (Tentative header) Structured Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0. Regularized Regression\n",
    "## 1.1 Ridge regression\n",
    "\n",
    "Recall that in the context of linear regression \n",
    "$$\\mathbf{y} = \\mathbf{X \\beta} + \\mathbf{\\epsilon}$$\n",
    "where $\\epsilon_i \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "Say $\\mathbf{y} \\in \\mathbb{R}^n$, the negative log-likelihood is \n",
    "$$L(\\mathbf{\\beta}|\\mathbf{X, y}) = \\dfrac{n}{2}\\log (2\\pi \\sigma^2) + \\dfrac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i -\\mathbf{x}_i^T\\mathbf{\\beta})^2$$\n",
    "Notice that the **MLE** is also the least squares soltion \n",
    "$$\\mathbf{\\beta}^* = \\underset{\\mathbf{\\beta}}{\\text{arg min}}\\sum_{i=1}^n(y_i -\\mathbf{x}_i^T\\mathbf{\\beta})^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9979467923453923"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=200, n_features=3, noise=4.0, random_state=0)\n",
    "reg = TheilSenRegressor(random_state=0).fit(X, y)\n",
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Huber regression is a robust regression method that uses the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which applies a linear loss (as opposed to quadratic loss) to samples that are classified as outliers (i.e., absolute error $> \\epsilon$)\n",
    "- In scikit-learn, the [`HuberRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor) minimizes the following loss function:\n",
    "\n",
    "$$\\min_{\\omega, \\sigma} \\sum_{i=1}^n(\\sigma + H_\\epsilon(\\frac{X_i\\omega - y_i}{\\sigma})\\sigma) + \\alpha ||\\omega||_2^2$$\n",
    "\n",
    "where $H_\\epsilon(z) = z^2 \\text{ if } |z| < \\epsilon$, and $H_\\epsilon(z) = \\epsilon|z| - \\epsilon^2 \\text{ otherwise}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}