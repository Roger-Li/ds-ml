{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization in Machine Learning Models\n",
    "\n",
    "**Yuanzhe(Roger) Li, 2020-01-07**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Outline\n",
    "- Regularized Regression\n",
    "    - Ridge Regression\n",
    "    - LASSO\n",
    "- (Tentative header) Structured Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularized Regression\n",
    "## Linear regression recap\n",
    "\n",
    "Recall that in the context of linear regression $\\mathbf{y} = \\mathbf{X \\beta} + \\mathbf{\\epsilon}$ where $\\epsilon_i \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "Say $\\mathbf{y} \\in \\mathbb{R}^n$, the negative log-likelihood is \n",
    "$$L(\\mathbf{\\beta}|\\mathbf{X, y}) = \\dfrac{n}{2}\\log (2\\pi \\sigma^2) + \\dfrac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i -\\mathbf{x}_i^T\\mathbf{\\beta})^2$$\n",
    "Notice that the **MLE** is also the least squares soltion \n",
    "$$\\mathbf{\\beta}^* = \\mathbf{\\beta}_{MLE} = \\mathbf{\\beta}_{LS} = \\underset{\\mathbf{\\beta}}{\\text{arg min}}\\sum_{i=1}^n(y_i -\\mathbf{x}_i^T\\mathbf{\\beta})^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear regression recap (continued)\n",
    "In matrix form \n",
    "$$\\begin{align}\n",
    "\\mathbf{\\beta}^* & = \\underset{\\mathbf{\\beta}}{\\text{arg min}} [(\\mathbf{y - X\\beta})^T(\\mathbf{y - X\\beta})] \\\\ \n",
    "& = \\underset{\\mathbf{\\beta}}{\\text{arg min}}[\\mathbf{y}^T\\mathbf{y} -2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T \\mathbf{X}^T\\mathbf{X \\beta}]  \\end{align}$$\n",
    "Denote $\\mathbf{e} = \\mathbf{y - X\\beta} $, and set \n",
    "\n",
    "$$\\dfrac{\\partial \\mathbf{e}^T \\mathbf{e}}{\\partial \\mathbf{\\beta}} = -2 \\mathbf{X}^T\\mathbf{y} + 2 \\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = \\mathbf{0}$$\n",
    "\n",
    "we have \n",
    "$$\n",
    "\\begin{align}\n",
    "& \\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = \\mathbf{X}^T\\mathbf{y} \\\\ \n",
    "\\Rightarrow \\quad & \\mathbf{\\beta}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Penalized loss function\n",
    "The loss function of regularized regression takes the following form:\n",
    "\n",
    "$$Loss(\\mathbf{\\beta}) = \\mathbf{E}(\\mathbf{\\beta}) + \\frac{\\lambda}{2} \\cdot \\mathbf{P}(\\mathbf{\\beta})$$\n",
    "where \n",
    "\n",
    "- $\\mathbf{E}(\\mathbf{\\beta}) = \\dfrac{1}{2}\\sum_{i=1}^n(y_i -\\mathbf{x}_i^T\\mathbf{\\beta})^2$ is the sum-of-squares error,\n",
    "- $\\mathbf{P}$ is a penalty function that aims at regularizing the unknown parameters, and \n",
    "- $\\lambda$ is a hyperparameter that controls the tradeoff between the two components of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression\n",
    "\n",
    "### Penalty\n",
    "One choice of $P_\\lambda$ that favors small regression coefficients is to penalize the sum of squares of the regression coefficients:\n",
    "\n",
    "$$P (\\mathbf{\\beta}) = \\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "and the whole loss function becomes\n",
    "\n",
    "$$Loss(\\mathbf{\\beta}) = \\frac{1}{2}\\sum_{i=1}^n(y_i -\\mathbf{x}_i^T\\mathbf{\\beta})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "which is known as ***ridge regression*** in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge regression\n",
    "\n",
    "### Solution \n",
    "\n",
    "Since the loss function of ridge regression is differentiable, it has a simple closed-form solution\n",
    "\n",
    "$$\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "The solution is similar to the least squares solution with an extra diagonal matrix (i.e., $\\lambda \\mathbf{I}$) to be inverted, which improves the numeric stability of the system when $\\mathbf{X}^T\\mathbf{X}$ is close to being non-invertible \n",
    "\n",
    "*Question: what does that mean in practice?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge regression\n",
    "\n",
    "### A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.51690508, -12.25821163])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate data\n",
    "np.random.seed(2)\n",
    "x1 = np.random.normal(size=20)\n",
    "x2 = np.random.normal(loc = x1, scale=0.01, size=20)\n",
    "y = np.random.normal(loc = 3 + x1 + x2, size=20)\n",
    "\n",
    "# Train an OLS model\n",
    "design_mat = np.hstack((np.array(x1).reshape(20,1), np.array(x2).reshape(20,1)))\n",
    "reg_ols = linear_model.LinearRegression()\n",
    "reg_ols.fit(design_mat, y)\n",
    "reg_ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.10585803, 1.08303544])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use ridge penalty\n",
    "reg_ridge = linear_model.Ridge(alpha = 1.0)\n",
    "reg_ridge.fit(design_mat, y)\n",
    "reg_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979467923453923"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=200, n_features=3, noise=4.0, random_state=0)\n",
    "reg = TheilSenRegressor(random_state=0).fit(X, y)\n",
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Huber regression is a robust regression method that uses the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which applies a linear loss (as opposed to quadratic loss) to samples that are classified as outliers (i.e., absolute error $> \\epsilon$)\n",
    "- In scikit-learn, the [`HuberRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor) minimizes the following loss function:\n",
    "\n",
    "$$\\min_{\\omega, \\sigma} \\sum_{i=1}^n(\\sigma + H_\\epsilon(\\frac{X_i\\omega - y_i}{\\sigma})\\sigma) + \\alpha ||\\omega||_2^2$$\n",
    "\n",
    "where $H_\\epsilon(z) = z^2 \\text{ if } |z| < \\epsilon$, and $H_\\epsilon(z) = \\epsilon|z| - \\epsilon^2 \\text{ otherwise}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
